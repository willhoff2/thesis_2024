{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2038fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim down imports to only neccesary \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import chardet\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import datasets_1 as datasets\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c957f873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample ID</th>\n",
       "      <th>Section depth (cm)</th>\n",
       "      <th>Cum depth (cm)</th>\n",
       "      <th>Age (BP)</th>\n",
       "      <th>δ13CVPDB (‰)</th>\n",
       "      <th>Total C (µg)</th>\n",
       "      <th>%TOC</th>\n",
       "      <th>δ15NAir (‰)</th>\n",
       "      <th>Total N (µg)</th>\n",
       "      <th>%N</th>\n",
       "      <th>C/N</th>\n",
       "      <th>Sample Weight (mg) from Sample List</th>\n",
       "      <th>Sample mass (ug)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20LVID-02A-19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>-26.801533</td>\n",
       "      <td>1374.888037</td>\n",
       "      <td>7.014735</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>131.049083</td>\n",
       "      <td>0.668618</td>\n",
       "      <td>10.491398</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20LVID-02A-22 5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>58</td>\n",
       "      <td>-24.681210</td>\n",
       "      <td>1464.275417</td>\n",
       "      <td>7.213179</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>135.527495</td>\n",
       "      <td>0.667623</td>\n",
       "      <td>10.804268</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20LVID-02A-26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>127</td>\n",
       "      <td>-24.237128</td>\n",
       "      <td>1101.039173</td>\n",
       "      <td>4.186461</td>\n",
       "      <td>0.947665</td>\n",
       "      <td>100.056127</td>\n",
       "      <td>0.380442</td>\n",
       "      <td>11.004215</td>\n",
       "      <td>26.3</td>\n",
       "      <td>26300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20LVID-02A-29 5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>201</td>\n",
       "      <td>-24.981566</td>\n",
       "      <td>1240.047631</td>\n",
       "      <td>2.966621</td>\n",
       "      <td>1.093422</td>\n",
       "      <td>108.097362</td>\n",
       "      <td>0.258606</td>\n",
       "      <td>11.471581</td>\n",
       "      <td>41.8</td>\n",
       "      <td>41800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20LVID-02A-33</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>287</td>\n",
       "      <td>-24.953268</td>\n",
       "      <td>1372.130105</td>\n",
       "      <td>4.848516</td>\n",
       "      <td>0.807939</td>\n",
       "      <td>120.690241</td>\n",
       "      <td>0.426467</td>\n",
       "      <td>11.369023</td>\n",
       "      <td>28.3</td>\n",
       "      <td>28300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sample ID  Section depth (cm)  Cum depth (cm)  Age (BP)  \\\n",
       "0    20LVID-02A-19                19.0             3.0        -4   \n",
       "1  20LVID-02A-22 5                22.5             6.5        58   \n",
       "2    20LVID-02A-26                26.0            10.0       127   \n",
       "3  20LVID-02A-29 5                29.5            13.0       201   \n",
       "4    20LVID-02A-33                33.0            17.0       287   \n",
       "\n",
       "   δ13CVPDB (‰)  Total C (µg)      %TOC  δ15NAir (‰)  Total N (µg)        %N  \\\n",
       "0    -26.801533   1374.888037  7.014735     0.232224    131.049083  0.668618   \n",
       "1    -24.681210   1464.275417  7.213179     0.196497    135.527495  0.667623   \n",
       "2    -24.237128   1101.039173  4.186461     0.947665    100.056127  0.380442   \n",
       "3    -24.981566   1240.047631  2.966621     1.093422    108.097362  0.258606   \n",
       "4    -24.953268   1372.130105  4.848516     0.807939    120.690241  0.426467   \n",
       "\n",
       "         C/N  Sample Weight (mg) from Sample List  Sample mass (ug)  \n",
       "0  10.491398                                 19.6             19600  \n",
       "1  10.804268                                 20.3             20300  \n",
       "2  11.004215                                 26.3             26300  \n",
       "3  11.471581                                 41.8             41800  \n",
       "4  11.369023                                 28.3             28300  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample ID</th>\n",
       "      <th>Section depth (cm)</th>\n",
       "      <th>Cum depth (cm)</th>\n",
       "      <th>Age (BP)</th>\n",
       "      <th>δ13CVPDB (‰)</th>\n",
       "      <th>Total C (µg)</th>\n",
       "      <th>%TOC</th>\n",
       "      <th>δ15NAir (‰)</th>\n",
       "      <th>Total N (µg)</th>\n",
       "      <th>%N</th>\n",
       "      <th>C/N</th>\n",
       "      <th>Sample Weight (mg) from Sample List</th>\n",
       "      <th>Sample mass (ug)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20LVID-02A-19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>-26.801533</td>\n",
       "      <td>1374.888037</td>\n",
       "      <td>7.014735</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>131.049083</td>\n",
       "      <td>0.668618</td>\n",
       "      <td>10.491398</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20LVID-02A-22 5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>58</td>\n",
       "      <td>-24.681210</td>\n",
       "      <td>1464.275417</td>\n",
       "      <td>7.213179</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>135.527495</td>\n",
       "      <td>0.667623</td>\n",
       "      <td>10.804268</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20LVID-02A-26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>127</td>\n",
       "      <td>-24.237128</td>\n",
       "      <td>1101.039173</td>\n",
       "      <td>4.186461</td>\n",
       "      <td>0.947665</td>\n",
       "      <td>100.056127</td>\n",
       "      <td>0.380442</td>\n",
       "      <td>11.004215</td>\n",
       "      <td>26.3</td>\n",
       "      <td>26300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20LVID-02A-29 5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>201</td>\n",
       "      <td>-24.981566</td>\n",
       "      <td>1240.047631</td>\n",
       "      <td>2.966621</td>\n",
       "      <td>1.093422</td>\n",
       "      <td>108.097362</td>\n",
       "      <td>0.258606</td>\n",
       "      <td>11.471581</td>\n",
       "      <td>41.8</td>\n",
       "      <td>41800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20LVID-02A-33</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>287</td>\n",
       "      <td>-24.953268</td>\n",
       "      <td>1372.130105</td>\n",
       "      <td>4.848516</td>\n",
       "      <td>0.807939</td>\n",
       "      <td>120.690241</td>\n",
       "      <td>0.426467</td>\n",
       "      <td>11.369023</td>\n",
       "      <td>28.3</td>\n",
       "      <td>28300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sample ID  Section depth (cm)  Cum depth (cm)  Age (BP)  \\\n",
       "0    20LVID-02A-19                19.0             3.0        -4   \n",
       "1  20LVID-02A-22 5                22.5             6.5        58   \n",
       "2    20LVID-02A-26                26.0            10.0       127   \n",
       "3  20LVID-02A-29 5                29.5            13.0       201   \n",
       "4    20LVID-02A-33                33.0            17.0       287   \n",
       "\n",
       "   δ13CVPDB (‰)  Total C (µg)      %TOC  δ15NAir (‰)  Total N (µg)        %N  \\\n",
       "0    -26.801533   1374.888037  7.014735     0.232224    131.049083  0.668618   \n",
       "1    -24.681210   1464.275417  7.213179     0.196497    135.527495  0.667623   \n",
       "2    -24.237128   1101.039173  4.186461     0.947665    100.056127  0.380442   \n",
       "3    -24.981566   1240.047631  2.966621     1.093422    108.097362  0.258606   \n",
       "4    -24.953268   1372.130105  4.848516     0.807939    120.690241  0.426467   \n",
       "\n",
       "         C/N  Sample Weight (mg) from Sample List  Sample mass (ug)  \n",
       "0  10.491398                                 19.6             19600  \n",
       "1  10.804268                                 20.3             20300  \n",
       "2  11.004215                                 26.3             26300  \n",
       "3  11.471581                                 41.8             41800  \n",
       "4  11.369023                                 28.3             28300  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sample ID', 'Section depth (cm)', 'Cum depth (cm)', 'Age (BP)',\n",
      "       'δ13CVPDB (‰)', 'Total C (µg)', '%TOC', 'δ15NAir (‰)', 'Total N (µg)',\n",
      "       '%N', 'C/N', 'Sample Weight (mg) from Sample List', 'Sample mass (ug)'],\n",
      "      dtype='object')\n",
      "Index(['sample_id', 'section_depth', 'cum_depth', 'age', 'δ13cvpdb', 'total_c',\n",
      "       '%toc', 'δ15nair', 'total_n', '%n', 'c/n',\n",
      "       'sample_weight_from_sample_list', 'sample_mass'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "lvid_o = pd.read_excel('../data/LVID_bulk_geochem.xlsx')\n",
    "lvid_o.head()\n",
    "\n",
    "lvid_o.columns = lvid_o.columns.str.strip()\n",
    "\n",
    "lvid_o.head()\n",
    "\n",
    "print(lvid_o.columns)\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(column_name):\n",
    "    # Remove text within parentheses\n",
    "    column_name = re.sub(r' \\([^)]*\\)', '', column_name)\n",
    "    # Replace spaces with underscores\n",
    "    column_name = column_name.replace(' ', '_')\n",
    "    \n",
    "    # Convert to lower case\n",
    "    return column_name.lower().strip()\n",
    "\n",
    "# Apply the function to each column name\n",
    "lvid_o.columns = [clean_column_name(col) for col in lvid_o.columns]\n",
    "\n",
    "# Now df has the cleaned column names\n",
    "print(lvid_o.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e073884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData:\n",
    "    def __init__(self, name, image):\n",
    "        self.name = name\n",
    "        self.image = image\n",
    "        self.start = None\n",
    "        self.end = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b920c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ILLUME-LVID20-1B-4B-1-A.jpg 4\n",
      "2ILLUME-LVID20-1B-2B-1-A.jpg 2\n",
      "1ILLUME-LVID20-1B-1B-1-A.jpg 1\n",
      "6ILLUME-LVID20-1A-5B-1-A.jpg 6\n",
      "8ILLUME-LVID20-1A-7B-1-A.jpg 8\n",
      "3ILLUME-LVID20-1B-3B-1-A.jpg 3\n",
      "5ILLUME-LVID20-1A-4B-1-A.jpg 5\n",
      "7ILLUME-LVID20-1A-6B-1-A.jpg 7\n",
      "9ILLUME-LVID20-1A-8B-1-A.jpg 9\n"
     ]
    }
   ],
   "source": [
    "full_LVID = []\n",
    "cropped_LVID = []\n",
    "\n",
    "def sortimg(img):\n",
    "    return img.name\n",
    "\n",
    "def load_and_display_images(folder_path, img_store):\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    for file in files:\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        obj_name = file[0]\n",
    "        print(file,obj_name)\n",
    "        # Check if the file is an image (you can add more extensions if needed)\n",
    "        if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Open the image\n",
    "            img = Image.open(file_path)\n",
    "            \n",
    "            new_img = ImageData(name = obj_name, image = img)\n",
    "            \n",
    "            # Display the image\n",
    "            #plt.imshow(img)\n",
    "            #plt.title(file)\n",
    "            #plt.show()\n",
    "            \n",
    "            img_store.append(new_img)\n",
    "\n",
    "# Replace 'your_folder_path' with the path to the folder containing your images\n",
    "\n",
    "load_and_display_images('img_data/LVID',full_LVID)\n",
    "## reverse = False because 1B-7B seems darkest so would make sense for it to be on the bottom\n",
    "full_LVID.sort(key=sortimg,reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201adf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cropping Unnecesary parts\n",
    "# Left, top, right, bottom\n",
    "\n",
    "## Cropping Unnecesary parts\n",
    "# Left, top, right, bottom\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 1. 3200 0 31075 1300\n",
    "    ## 17-156 cm\n",
    "start = 3200\n",
    "end = 31075\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[0].image.crop(box)\n",
    "full_LVID[0].start = start\n",
    "full_LVID[0].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[0].name,cropped_image))\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 2. 1400 0 15525 1300\n",
    "    ## 8-78 cm \n",
    "start = 1400\n",
    "end = 15525\n",
    "box = (start, 0, end, 1300)\n",
    "full_LVID[1].start = start\n",
    "full_LVID[1].end = end\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[1].image.crop(box)\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[1].name,cropped_image))\n",
    "\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 3. 1200 0 21850 1300\n",
    "    ## 7-109.5 cm\n",
    "start = 1200\n",
    "end = 21850\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[2].image.crop(box)\n",
    "full_LVID[2].start = start\n",
    "full_LVID[2].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[2].name,cropped_image))\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 4. 1000 0 21650 1300\n",
    "    ## 6-108.5 cm\n",
    "start = 1000\n",
    "end = 21650\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[3].image.crop(box)\n",
    "full_LVID[3].start = start\n",
    "full_LVID[3].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[3].name,cropped_image))\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 5. 1000 0 20600 1300\n",
    "    ## 6-103.5 cm\n",
    "start = 1000\n",
    "end = 20600\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[4].image.crop(box)\n",
    "full_LVID[4].start = start\n",
    "full_LVID[4].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[4].name,cropped_image))\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 6. 1000 0 21150 1300\n",
    "    ## 6-106 cm\n",
    "start = 1000\n",
    "end = 21150\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[5].image.crop(box)\n",
    "full_LVID[5].start = start\n",
    "full_LVID[5].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[5].name,cropped_image))\n",
    "\n",
    "## This is only guessing, do something better potentially. More fine grain\n",
    "## 7. 900 0 21900 1300\n",
    "    ## 5.5-110 cm\n",
    "start = 900\n",
    "end = 21900\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[6].image.crop(box)\n",
    "full_LVID[6].start = start\n",
    "full_LVID[6].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[6].name,cropped_image))\n",
    "\n",
    "## 8. 900 0 21900 1300\n",
    "    ## 5.5-110 cm\n",
    "start = 900\n",
    "end = 21900\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[6].image.crop(box)\n",
    "full_LVID[7].start = start\n",
    "full_LVID[7].end = end\n",
    "\n",
    "# Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[7].name,cropped_image))\n",
    "\n",
    "## 9. 900 0 21900 1300\n",
    "    ## 5.5-110 cm\n",
    "start = 900\n",
    "end = 21900\n",
    "box = (start, 0, end, 1300)\n",
    "\n",
    "# Crop the image\n",
    "cropped_image = full_LVID[6].image.crop(box)\n",
    "full_LVID[8].start = start\n",
    "full_LVID[8].end = end\n",
    "\n",
    "## Display the cropped image\n",
    "#plt.imshow(cropped_image)\n",
    "#plt.axis('off')  # This hides the axis\n",
    "#plt.show()\n",
    "\n",
    "cropped_LVID.append(ImageData(full_LVID[8].name,cropped_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41aa5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    " ################################################################################################################                       \n",
    "                            ####################### TARGET LABEL #######################\n",
    "  ################################################################################################################ \n",
    "\n",
    "\n",
    "#['sample_id', 'section_depth', 'cum_depth', 'age', 'δ13cvpdb', 'total_c',\n",
    "        #'%toc', 'δ15nair', 'total_n', '%n', 'c/n',\n",
    "       #'sample_weight_from_sample_list', 'sample_mass']\n",
    "\n",
    "target = '%toc'\n",
    "scaled = True\n",
    "sample_df = lvid_o\n",
    "br = False\n",
    "if br:\n",
    "    target = 'MBT'\n",
    "    sample_df = df\n",
    "\n",
    "if scaled:\n",
    "    target_col = target + '_scaled'\n",
    "else:\n",
    "    target_col = target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 997 is good chunk size\n",
    "\n",
    "## Images loaded, tabular loaded, Link together\n",
    "\n",
    "\n",
    "\n",
    "full_depth = 0\n",
    "chunk_size = 997\n",
    "chunks = []\n",
    "chunk_labels = []\n",
    "\n",
    "## Pixels to cm\n",
    "    ## To make sure our depths are accurate\n",
    "p_to_cm = 5/997\n",
    "\n",
    "\n",
    "if scaled:\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler to the target column and transform it\n",
    "    sample_df[target_col] = scaler.fit_transform(sample_df[[target]])\n",
    "\n",
    "\n",
    "\n",
    "j = 0\n",
    "for i,object in enumerate(full_LVID):\n",
    "    full_end = object.end\n",
    "    start_pixel = object.start\n",
    "    end_pixel = object.start + chunk_size\n",
    "    while end_pixel <= full_end:\n",
    "        box = (start_pixel, 0, end_pixel, 1300)\n",
    "            \n",
    "        ## Adding on labels\n",
    "        section_data =  sample_df[( sample_df['cum_depth'] >= full_depth) & \n",
    "                        ( sample_df['cum_depth'] < (full_depth+5))]\n",
    "\n",
    "        # Calculate the average target value for the segment\n",
    "        average_target = section_data[target_col].mean()\n",
    "        #print(f\"target for section {full_depth} to {full_depth + 5}: {average_target}\\n\")\n",
    "        if len(section_data) > 0:\n",
    "            chunk_labels.append(average_target)\n",
    "            chunk = object.image.crop(box)\n",
    "\n",
    "            chunks.append(chunk) \n",
    "\n",
    "        ## Looks good, dont need to display for now\n",
    "        # Display the chunk\n",
    "        #plt.figure()  # Create a new figure for each chunk\n",
    "        #plt.imshow(chunk)\n",
    "        #plt.axis('off')  # Hide the axis\n",
    "        #plt.title(f'Chunk {j+1}')\n",
    "        #plt.show()\n",
    "        \n",
    "        \n",
    "        ## While loop Increments\n",
    "        start_pixel += chunk_size\n",
    "        end_pixel += chunk_size\n",
    "        j += 1\n",
    "        if end_pixel > full_end:\n",
    "            full_depth += (full_end - (end_pixel - chunk_size)) * p_to_cm\n",
    "        else:\n",
    "            full_depth += 5\n",
    "            \n",
    "    #print(f\"\\n end of image {i}\\n\")\n",
    "        \n",
    "print(len(chunks))       \n",
    "        \n",
    " # Convert your lists to numpy arrays\n",
    "images_array = np.array(chunks)\n",
    "labels_array = np.array(chunk_labels)\n",
    "\n",
    "# Assuming NaN is represented as np.nan in labels_array\n",
    "# Create boolean masks\n",
    "nan_mask = np.isnan(labels_array)\n",
    "non_nan_mask = ~nan_mask\n",
    "\n",
    "# Filter the arrays\n",
    "images_test = images_array[nan_mask]\n",
    "images_known = images_array[non_nan_mask]\n",
    "labels_test = labels_array[nan_mask]\n",
    "labels_known = labels_array[non_nan_mask]\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "inputs = feature_extractor(images=images_known, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs['pixel_values'][0].shape)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pixel_values, labels):\n",
    "        self.pixel_values = pixel_values\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixel_values[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Assuming your pixel_values are already a tensor in the shape [num_images, channels, height, width]\n",
    "# and labels is a list or a 1-D tensor of label values:\n",
    "pixel_values_tensor = inputs['pixel_values'] \n",
    "labels_tensor = torch.tensor(labels_known, dtype=torch.float32)\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(pixel_values_tensor, labels_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(labels_tensor)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Subset for train and validation=\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Create a dataloader for both the training and validation sets\n",
    "    ## 5 seems like a fine batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44235cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pixel_values, labels):\n",
    "        self.pixel_values = pixel_values\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixel_values[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_dataset(target,sample_df,scaled=True):\n",
    "    if scaled:\n",
    "        target_col = target + '_scaled'\n",
    "    else:\n",
    "        target_col = target\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    full_depth = 0\n",
    "    chunk_size = 997\n",
    "    chunks = []\n",
    "    chunk_labels = []\n",
    "\n",
    "    ## Pixels to cm\n",
    "        ## To make sure our depths are accurate\n",
    "    p_to_cm = 5/997\n",
    "\n",
    "\n",
    "    if scaled:\n",
    "        # Initialize the StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler to the target column and transform it\n",
    "        sample_df[target_col] = scaler.fit_transform(sample_df[[target]])\n",
    "\n",
    "\n",
    "\n",
    "    j = 0\n",
    "    for i,object in enumerate(full_LVID):\n",
    "        full_end = object.end\n",
    "        start_pixel = object.start\n",
    "        end_pixel = object.start + chunk_size\n",
    "        while end_pixel <= full_end:\n",
    "            box = (start_pixel, 0, end_pixel, 1300)\n",
    "\n",
    "            ## Adding on labels\n",
    "            section_data =  sample_df[( sample_df['cum_depth'] >= full_depth) & \n",
    "                            ( sample_df['cum_depth'] < (full_depth+5))]\n",
    "\n",
    "            # Calculate the average target value for the segment\n",
    "            average_target = section_data[target_col].mean()\n",
    "            if len(section_data) > 0:\n",
    "                chunk_labels.append(average_target)\n",
    "                chunk = object.image.crop(box)\n",
    "\n",
    "                chunks.append(chunk) \n",
    "\n",
    "\n",
    "            ## While loop Increments\n",
    "            start_pixel += chunk_size\n",
    "            end_pixel += chunk_size\n",
    "            j += 1\n",
    "            if end_pixel > full_end:\n",
    "                full_depth += (full_end - (end_pixel - chunk_size)) * p_to_cm\n",
    "            else:\n",
    "                full_depth += 5\n",
    "     \n",
    "\n",
    "     # Convert your lists to numpy arrays\n",
    "    images_array = np.array(chunks)\n",
    "    labels_array = np.array(chunk_labels)\n",
    "\n",
    "    # Assuming NaN is represented as np.nan in labels_array\n",
    "    # Create boolean masks\n",
    "    nan_mask = np.isnan(labels_array)\n",
    "    non_nan_mask = ~nan_mask\n",
    "\n",
    "    # Filter the arrays\n",
    "    images_test = images_array[nan_mask]\n",
    "    images_known = images_array[non_nan_mask]\n",
    "    labels_test = labels_array[nan_mask]\n",
    "    labels_known = labels_array[non_nan_mask]\n",
    "\n",
    "\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "    inputs = feature_extractor(images=images_known, return_tensors=\"pt\")\n",
    "    \n",
    "        # Assuming your pixel_values are already a tensor in the shape [num_images, channels, height, width]\n",
    "    # and labels is a list or a 1-D tensor of label values:\n",
    "    pixel_values_tensor = inputs['pixel_values'] \n",
    "    labels_tensor = torch.tensor(labels_known, dtype=torch.float32)\n",
    "\n",
    "    # Create the custom dataset\n",
    "    dataset = CustomDataset(pixel_values_tensor, labels_tensor)\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(range(len(labels_tensor)), test_size=0.2, random_state=42)\n",
    "\n",
    "    # Subset for train and validation=\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "    # Create a dataloader for both the training and validation sets\n",
    "        ## 5 seems like a fine batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "449193fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "## Pretrained model\n",
    "\n",
    "class CustomViTEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomViTEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Extract the necessary layers from the original model\n",
    "        self.embeddings = original_model.vit.embeddings  #.patch_embeddings\n",
    "        self.encoder_layer_0 = original_model.vit.encoder.layer[0]\n",
    "        self.encoder_layer_1 = original_model.vit.encoder.layer[1]\n",
    "        \n",
    "        # Assume a square grid of patches to reshape the sequence of patches back into a 2D grid\n",
    "            ## image: 224x224 ; patch size: 16x16 --> 14x14 \n",
    "        self.num_patches_side = 14\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the embeddings layer\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # Pass the result through the first and second encoder layers\n",
    "        x = self.encoder_layer_0(x)[0]  # [0] to get the hidden states\n",
    "        x = self.encoder_layer_1(x)[0]  # [0] to get the hidden states\n",
    "        \n",
    "        # x is now the sequence of embeddings for the patches\n",
    "            # The output x will be a sequence of embeddings, one for each patch of the input images.\n",
    "            # If you're looking for a single vector representation per image, typically the class token embedding (the first token) is used. \n",
    "            # If the model doesn't use a class token, you might need to apply a different pooling strategy over the patch embeddings.\n",
    "        \n",
    "        ## Updating to reshape\n",
    "        \n",
    "        # Before reshaping, x is in shape [batch_size, num_patches+1, embedding_dim]\n",
    "        # We discard the first token which is used for classification in the original ViT model\n",
    "        x = x[:, 1:, :]  # Now in shape [batch_size, num_patches, embedding_dim]\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches_side, num_patches_side, embedding_dim]\n",
    "        x = x.reshape(-1, self.num_patches_side, self.num_patches_side, x.size(-1))\n",
    "\n",
    "        # Permute to get [batch_size, embedding_dim, num_patches_side, num_patches_side]\n",
    "        # This is a pseudo-spatial 2D grid, where embedding_dim becomes the channel dimension\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Load the pre-trained ViT model\n",
    "pretrained_vit = ViTForImageClassification.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "\n",
    "## Freeze params\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create model w first three layers and create embedding\n",
    "custom_model = CustomViTEmbeddingModel(pretrained_vit)\n",
    "embeddings = custom_model(inputs['pixel_values']) \n",
    "\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eac34f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_and_r2(loader, model, scaler):\n",
    "    model.eval()\n",
    "    targets, predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            preds = model(embeddings)  # Pass embeddings to the CNN\n",
    "            predictions.extend(preds.view(-1).tolist())\n",
    "            targets.extend(labels.tolist())\n",
    "\n",
    "    # Scale the targets using the provided scaler\n",
    "    targets_scaled = scaler.transform(np.array(targets).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert to tensors\n",
    "    predictions = torch.tensor(predictions)\n",
    "    targets_scaled = torch.tensor(targets_scaled)\n",
    "\n",
    "    # Calculate RMSE on scaled targets\n",
    "    rmse_value = torch.sqrt(nn.functional.mse_loss(predictions, targets_scaled))\n",
    "\n",
    "    # Calculate R^2 on scaled targets\n",
    "    r2_value = r2_score(targets_scaled, predictions)\n",
    "\n",
    "    return rmse_value.item(), r2_value\n",
    "\n",
    "def count_layers(model):\n",
    "    layer_count = 0\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Module):\n",
    "            # If the child is a container module (like nn.Sequential), recursively count its children\n",
    "            if isinstance(child, nn.Sequential) or isinstance(child, nn.ModuleList):\n",
    "                layer_count += count_layers(child)\n",
    "            else:\n",
    "                layer_count += 1\n",
    "    return layer_count\n",
    "\n",
    "def count_conv2d_layers(model):\n",
    "    layer_count = 0\n",
    "    for child in model.children():\n",
    "        # Check if the child is a convolutional layer\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            layer_count += 1\n",
    "        # If the child is a container module, recursively count its Conv2d children\n",
    "        elif isinstance(child, nn.Sequential) or isinstance(child, nn.ModuleList):\n",
    "            layer_count += count_conv2d_layers(child)\n",
    "    return layer_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965907a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preventing model arch from printing everytime\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e696da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search\n",
    "\n",
    "class GridRegressionCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, layer_sizes, use_dropout, use_batch_norm, act_func):\n",
    "        super(GridRegressionCNN, self).__init__()\n",
    "        \n",
    "        # Example architecture with dynamic dropout and batch normalization\n",
    "        layers = []\n",
    "        in_channels = embedding_dim\n",
    "\n",
    "        for size in layer_sizes:\n",
    "            layers.append(nn.Conv2d(in_channels, size, kernel_size=3, padding=1))\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(size))\n",
    "            \n",
    "            layers.append(act_func)\n",
    "\n",
    "            if use_dropout:\n",
    "                ## Changed to 0.1\n",
    "                layers.append(nn.Dropout(p=0.1))\n",
    "\n",
    "            in_channels = size\n",
    "\n",
    "        # Add the adaptive pooling layer\n",
    "        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "        # Convert the list of layers into a Sequential module\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # The final fully connected layer\n",
    "        self.fc = nn.Linear(in_features=layer_sizes[-1], out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layer for regression\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a9f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_evaluate_model(params, train_loader,val_loader,scaler, df):\n",
    "    model = GridRegressionCNN(embedding_dim=192, layer_sizes=params['layer_sizes'], use_dropout=params['use_dropout'], use_batch_norm=params['use_batch_norm'],act_func = params['activations'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Reset metrics for each training session\n",
    "    train_losses = []\n",
    "    train_rmses = []\n",
    "    val_rmses = []\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        #print(f'epoch {epoch} of {epochs}')\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            predictions = model(embeddings)  # Pass embeddings to the CNN\n",
    "            loss = loss_function(predictions.squeeze(), labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Append average loss and RMSE for this epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_rmse, _ = calculate_rmse_and_r2(train_loader, model, scaler)\n",
    "        val_rmse, _ = calculate_rmse_and_r2(val_loader, model, scaler)\n",
    "        train_rmses.append(train_rmse)\n",
    "        val_rmses.append(val_rmse)\n",
    "    print(f'PERFORMANCE: \\n params: {params}, \\n val_rmse: {val_rmses[-1]} \\n')\n",
    "    range_target = np.max(df[target]) - np.min(df[target])\n",
    "    if scaled:\n",
    "        normal_rmse = scaler.inverse_transform([[val_rmses[-1]]])[0, 0]\n",
    "        rmse_perc = (normal_rmse / range_target) * 100\n",
    "    else:\n",
    "        rmse_perc = (val_rmses[-1] / range_target) * 100\n",
    "\n",
    "    return {'train_rmse': train_rmses[-1], 'val_rmse': val_rmses[-1], 'rmse_perc': rmse_perc}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f7c81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_grid_search(lake,target,param_grid, train_loader, val_loader,scaler, df):\n",
    "    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    results = []\n",
    "    for params in all_params:\n",
    "        epochs = params['epochs']\n",
    "        print(f'epochs {epochs}')\n",
    "        performance = train_evaluate_model(params, train_loader, val_loader,scaler, df)\n",
    "        results.append({'params': params, 'performance': performance})\n",
    "\n",
    "    # Convert results to a DataFrame for easier handling\n",
    "    results_df = pd.DataFrame([{\n",
    "        'epochs': r['params']['epochs'],\n",
    "        'learning_rate': r['params']['learning_rate'],\n",
    "        'layer_sizes': r['params']['layer_sizes'],\n",
    "        'use_dropout': r['params']['use_dropout'],\n",
    "        'use_batch_norm': r['params']['use_batch_norm'],\n",
    "        'activation': r['params']['activations'],\n",
    "        'train_rmse': r['performance']['train_rmse'],\n",
    "        'val_rmse': r['performance']['val_rmse'],\n",
    "        'RMSE %': r['performance']['rmse_perc']\n",
    "    } for r in results])\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    target = target.replace(\"/\", \"-\")\n",
    "    title = \"grid_results/\" + lake + \"/\" + target + \"_\" + date.today().strftime(\"%d%m%Y_%H%M%S\") + '_grid_search_results.csv'\n",
    "    results_df.to_csv(title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcf85353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-07, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.0914538176226927 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-07, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.1961551379484296 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-07, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.5046645566127705 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.240777764323273 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.5968856189093388 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.6742356972082106 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.486874507390765 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.515859528787298 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-06, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.3121786478689734 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.8661550411334478 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.8509400328358394 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 5e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.747177478372943 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.561391712810518 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.629280202215583 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.6320495932460934 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.7355579260218423 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.6657262213880095 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.8492031301100136 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.3783391359022867 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.2851429497059654 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.0001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.7906191204244184 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.43383262808831 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.557238592124955 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.005, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.548884234187779 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.6731363237530443 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.625882526300175 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.001, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.513551865206338 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.808965458608026 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.4604618154466738 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.05, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.62593379393992 \n",
      "\n",
      "epochs 50\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.01, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 50}, \n",
      " val_rmse: 2.76845435660424 \n",
      "\n",
      "epochs 100\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.01, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 100}, \n",
      " val_rmse: 2.551132538743572 \n",
      "\n",
      "epochs 200\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 0.01, 'layer_sizes': [128, 64], 'use_dropout': True, 'use_batch_norm': True, 'activations': ReLU(), 'epochs': 200}, \n",
      " val_rmse: 2.485433150882989 \n",
      "\n",
      "FINISHED %toc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Warnings off\n",
    "## Preventing model arch from printing everytime\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#['sample_id', 'section_depth', 'cum_depth', 'age', 'δ13cvpdb', 'total_c',\n",
    "        #'%toc', 'δ15nair', 'total_n', '%n', 'c/n',\n",
    "       #'sample_weight_from_sample_list', 'sample_mass'] MBT\n",
    "        \n",
    "        ## lvid_o or df\n",
    "## Create Training set for each in for loop\n",
    "\n",
    "## have a way to turn off scaling?\n",
    "## TOC not included for now, did it manually before\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0000001, 0.000005, 0.000001, 0.00005, 0.00001, 0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01],\n",
    "    'layer_sizes': [[128,64]],\n",
    "    'use_dropout': [True],\n",
    "    'use_batch_norm': [True],\n",
    "    'activations': [nn.ReLU()],\n",
    "    'epochs': [50,100,200]\n",
    "}\n",
    "\n",
    "targets = ['%toc']\n",
    "lake = 'LVID'\n",
    "train_loader,val_loader,scaler = create_dataset(target, lvid_o)\n",
    "for target in targets:\n",
    "    if target == \"MBT\":\n",
    "        train_loader,val_loader,scaler = create_dataset(target, df)\n",
    "    else if \"MBT\" in targets:\n",
    "        train_loader,val_loader,scaler = create_dataset(target, lvid_o)\n",
    "    \n",
    "        \n",
    "    ## Do grid search in function\n",
    "    manual_grid_search(lake, target, param_grid, train_loader, val_loader,scaler, lvid_o)\n",
    "    print(f'FINISHED {target}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

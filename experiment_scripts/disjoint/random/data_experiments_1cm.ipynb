{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a27e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim down imports to only neccesary \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, ViTModel\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "\n",
    "## Preprocessing loc\n",
    "preprocess_dir = '/Users/willhoff/Desktop/thesis_2024/preprocessing'\n",
    "sys.path.append(preprocess_dir)\n",
    "from simple_datasets import load_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretrained model\n",
    "\n",
    "class CustomViTEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomViTEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Extract the necessary layers from the original model\n",
    "        self.embeddings = original_model.vit.embeddings  #.patch_embeddings\n",
    "        self.encoder_layer_0 = original_model.vit.encoder.layer[0]\n",
    "        self.encoder_layer_1 = original_model.vit.encoder.layer[1]\n",
    "        \n",
    "        # Assume a square grid of patches to reshape the sequence of patches back into a 2D grid\n",
    "            ## image: 224x224 ; patch size: 16x16 --> 14x14 \n",
    "        self.num_patches_side = 14\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the embeddings layer\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # Pass the result through the first and second encoder layers\n",
    "        x = self.encoder_layer_0(x)[0]  # [0] to get the hidden states\n",
    "        x = self.encoder_layer_1(x)[0]  # [0] to get the hidden states\n",
    "        \n",
    "        # x is now the sequence of embeddings for the patches\n",
    "            # The output x will be a sequence of embeddings, one for each patch of the input images.\n",
    "            # If you're looking for a single vector representation per image, typically the class token embedding (the first token) is used. \n",
    "            # If the model doesn't use a class token, you might need to apply a different pooling strategy over the patch embeddings.\n",
    "        \n",
    "        ## Updating to reshape\n",
    "        \n",
    "        # Before reshaping, x is in shape [batch_size, num_patches+1, embedding_dim]\n",
    "        # We discard the first token which is used for classification in the original ViT model\n",
    "        x = x[:, 1:, :]  # Now in shape [batch_size, num_patches, embedding_dim]\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches_side, num_patches_side, embedding_dim]\n",
    "        x = x.reshape(-1, self.num_patches_side, self.num_patches_side, x.size(-1))\n",
    "\n",
    "        # Permute to get [batch_size, embedding_dim, num_patches_side, num_patches_side]\n",
    "        # This is a pseudo-spatial 2D grid, where embedding_dim becomes the channel dimension\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding in CNN component\n",
    "\n",
    "\n",
    "## CNN for regression\n",
    "class RegressionCNN(nn.Module):\n",
    "    ## Maybe try messing w/ kernel_size, padding and stride?\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(RegressionCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "        # Second convolutional layer\n",
    "            ## Same amount? Dropout should help it to learn other things\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "        # Third convolutional layer -- new\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        # Implementing dropout\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Adaptive pooling layer to pool down to 1x1\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Final fully connected layer for regressin\n",
    "        self.fc = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, batch norm, and activation\n",
    "        x = self.sig1(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolution, batch norm, and activation\n",
    "        x = self.sig2(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Apply third convolution, batch norm, and activation -- new\n",
    "        x = self.sig3(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pool the output from the convolutional layers\n",
    "        x = self.adapt_pool(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layer for regression\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def calculate_rmse_and_r2(loader, model, scaler):\n",
    "    model.eval()\n",
    "    targets, predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            preds = cnn_model(embeddings)  # Pass embeddings to the CNN\n",
    "            predictions.extend(preds.view(-1).tolist())\n",
    "            targets.extend(labels.tolist())\n",
    "\n",
    "    # Scale the targets using the provided scaler\n",
    "    targets_scaled = scaler.transform(np.array(targets).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert to tensors\n",
    "    predictions = torch.tensor(predictions)\n",
    "    targets_scaled = torch.tensor(targets_scaled)\n",
    "\n",
    "    # Calculate RMSE on scaled targets\n",
    "    rmse_value = torch.sqrt(nn.functional.mse_loss(predictions, targets_scaled))\n",
    "\n",
    "    # Calculate R^2 on scaled targets\n",
    "    r2_value = r2_score(targets_scaled, predictions)\n",
    "\n",
    "    return rmse_value.item(), r2_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431113a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, custom_model, scaler, return_depths=False, return_source = False):\n",
    "    model.eval()\n",
    "    predictions, actuals, depths,sources = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels, batch_depths, batch_source = batch\n",
    "            embeddings = custom_model(images)\n",
    "            preds = model(embeddings)\n",
    "            predictions.extend(preds.view(-1).cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "            if return_depths:\n",
    "                depths.extend(batch_depths.cpu().numpy())\n",
    "            if return_source:\n",
    "                sources.extend(batch_source)\n",
    "\n",
    "    # Scale back if necessary\n",
    "    if scaler:\n",
    "        predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "        actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n",
    "\n",
    "    return sources, depths, actuals, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, title, save_path):\n",
    "    plt.figure(figsize=(10, 9))\n",
    "\n",
    "    # Convert to numpy arrays if not already\n",
    "    train_depths = np.array(train_depths)\n",
    "    train_actuals = np.array(train_actuals)\n",
    "    train_sources = np.array(train_sources)\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    test_depths = np.array(test_depths)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "    test_sources = np.array(test_sources)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "\n",
    "    # Create a grid with 2 rows and 1 column, \n",
    "    # the first row is twice the height of the second row\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1]) \n",
    "\n",
    "    # Main plot for actual values\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter train and test data\n",
    "        train_mask = (train_sources == lake)\n",
    "        test_mask = (test_sources == lake)\n",
    "        \n",
    "        #print(f\"{lake} - Train mask sum: {np.sum(train_mask)}, Test mask sum: {np.sum(test_mask)}\")\n",
    "\n",
    "        # Check and concatenate\n",
    "        if np.any(train_mask) or np.any(test_mask):\n",
    "            all_depths = np.concatenate([train_depths[train_mask], test_depths[test_mask]])\n",
    "            all_actuals = np.concatenate([train_actuals[train_mask], test_actuals[test_mask]])\n",
    "            #print(\"Check\\n\", all_depths, \"\\n\", all_actuals)\n",
    "            sorted_indices = np.argsort(all_depths)\n",
    "            plt.plot(all_depths[sorted_indices], all_actuals[sorted_indices], label=f'{lake} Actual', linestyle='-', marker='o')\n",
    "\n",
    "    ax1.set_xlabel('Sediment Depth')\n",
    "    ax1.set_ylabel('Target Variable')\n",
    "    ax1.set_title(title)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Subplot for residuals\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter train and test data for residuals\n",
    "        train_mask = (train_sources == lake)\n",
    "        test_mask = (test_sources == lake)\n",
    "\n",
    "        # Check and plot residuals\n",
    "        if np.any(train_mask):\n",
    "            train_depths_lake = train_depths[train_mask]\n",
    "            train_residuals = abs(train_actuals[train_mask] - train_predictions[train_mask])\n",
    "            sorted_indices = np.argsort(train_depths_lake)\n",
    "            plt.plot(train_depths_lake[sorted_indices], train_residuals[sorted_indices], label=f'{lake} Train Residual', linestyle='-', marker='x')\n",
    "\n",
    "        if np.any(test_mask):\n",
    "            test_depths_lake = test_depths[test_mask]\n",
    "            test_residuals = abs(test_actuals[test_mask] - test_predictions[test_mask])\n",
    "            sorted_indices = np.argsort(test_depths_lake)\n",
    "            plt.plot(test_depths_lake[sorted_indices], test_residuals[sorted_indices], label=f'{lake} Test Residual', linestyle='--', marker='o')\n",
    "\n",
    "    ax2.set_xlabel('Sediment Depth')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def old_plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, title, save_path):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Convert sources to numpy arrays if they are not already\n",
    "    train_depths = np.array(train_depths)\n",
    "    train_actuals = np.array(train_actuals)\n",
    "    train_sources = np.array(train_sources)\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    test_depths = np.array(test_depths)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_sources = np.array(test_sources)\n",
    "\n",
    "    check = 0\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter and check train data\n",
    "        lake_train_mask = train_sources == lake\n",
    "        if np.any(lake_train_mask):\n",
    "            lake_train_depths = train_depths[lake_train_mask]\n",
    "            lake_train_actuals = train_actuals[lake_train_mask]\n",
    "            sorted_train_indices = np.argsort(lake_train_depths)\n",
    "            plt.plot(lake_train_depths[sorted_train_indices], lake_train_actuals[sorted_train_indices], label=f'{lake} Train', linestyle='-', marker='o')\n",
    "            check += 1\n",
    "\n",
    "        # Filter and check test data\n",
    "        lake_test_mask = test_sources == lake\n",
    "        if np.any(lake_test_mask):\n",
    "            lake_test_depths = test_depths[lake_test_mask]\n",
    "            lake_test_actuals = test_actuals[lake_test_mask]\n",
    "            lake_test_predictions = test_predictions[lake_test_mask]\n",
    "            sorted_test_indices = np.argsort(lake_test_depths)\n",
    "            plt.plot(lake_test_depths[sorted_test_indices], lake_test_actuals[sorted_test_indices], label=f'{lake} Test Actual', linestyle='-', marker='o')\n",
    "            plt.plot(lake_test_depths[sorted_test_indices], lake_test_predictions[sorted_test_indices], label=f'{lake} Test Predicted', linestyle='--', marker='x')\n",
    "            check += 1\n",
    "            \n",
    "    if check == 0:\n",
    "        return \"No data to plot!\"\n",
    "    \n",
    "    plt.xlabel('Sediment Depth')\n",
    "    plt.ylabel('Target Variable')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23374e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_loader, test_loader, cnn_model, custom_model, scaler, num_epochs=100, lr=0.001):\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training steps...\n",
    "        cnn_model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels, _, _ = batch\n",
    "            embeddings = custom_model(images)\n",
    "            predictions = cnn_model(embeddings)\n",
    "            loss = loss_function(predictions.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    train_sources, train_depths, train_actuals, train_predictions = evaluate_model(cnn_model, train_loader, custom_model, scaler, return_depths=True, return_source = True)\n",
    "    test_sources, test_depths, test_actuals, test_predictions = evaluate_model(cnn_model, test_loader, custom_model, scaler, return_depths=True, return_source = True)\n",
    "\n",
    "    return train_depths, train_actuals, train_sources, train_predictions, test_depths, test_actuals, test_predictions, test_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Define the combinations\n",
    "combinations = [\n",
    "    #(\"lvid\", \"lvid\"),\n",
    "    #(\"lvid\", \"svid\"),\n",
    "    #(\"lvid\", \"both\"),\n",
    "    #(\"svid\", \"lvid\"),\n",
    "    (\"svid\", \"svid\"),\n",
    "    #(\"svid\", \"both\"),\n",
    "    #(\"both\", \"lvid\"),\n",
    "    #(\"both\", \"svid\"),\n",
    "    #(\"both\", \"both\")\n",
    "]\n",
    "targets = ['%TOC']\n",
    "# Load the pre-trained ViT model\n",
    "pretrained_vit = ViTForImageClassification.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "\n",
    "## Freeze params\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create model w first three layers and create embedding\n",
    "custom_model = CustomViTEmbeddingModel(pretrained_vit)\n",
    "\n",
    "for target in targets:\n",
    "    for train_set, test_set in combinations:\n",
    "        cnn_model = RegressionCNN(embedding_dim=192)\n",
    "        print(f\"Running experiment for Target: {target}, Train: {train_set}, Test: {test_set}\")\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, _, train_scaler = load_data(target, lake=train_set, set=\"full\", scaled=False, sediment_width = 1)\n",
    "        _, test_loader, _ = load_data(target, lake=test_set, set=\"full\", scaled=False, sediment_width = 1)\n",
    "        \n",
    "        train_depths, train_actuals, train_sources, train_predictions, test_depths, test_actuals, test_predictions, test_sources = run_experiment(train_loader, test_loader, cnn_model, custom_model, train_scaler, num_epochs=100, lr=0.001)\n",
    "        #print(train_sources,test_sources)\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H-%M-%S\")\n",
    "        save_dir = 'data_exp/1cm/plots'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = f'{target}_{train_set}train{test_set}_test_{timestamp}.png'\n",
    "        plot_title = f\"{target} Predictions (Train: {train_set}, Test: {test_set})\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        plot_results(train_depths, train_actuals, train_predictions, train_sources,  test_depths, test_actuals, test_predictions, test_sources, plot_title, save_path)\n",
    "        filename = f'OLD{target}_{train_set}train{test_set}_test_{timestamp}.png'\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        old_plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, plot_title, save_path)\n",
    "        # Calculate performance metrics\n",
    "        rmse_value = np.sqrt(mean_squared_error(test_actuals, test_predictions))\n",
    "\n",
    "        # Add the results to the results dataframe\n",
    "        results.append({\n",
    "            \"Train Set\": train_set,\n",
    "            \"Test Set\": test_set,\n",
    "            \"Target\": target,\n",
    "            \"RMSE\": rmse_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H-%M-%S\")\n",
    "save_dir = 'data_exp/1cm/results'\n",
    "\n",
    "# Saving the plot\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "filename = f'experiment_results_{timestamp}.csv'\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(os.path.join(save_dir, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls \"/Users/willhoff/Desktop/thesis_2024/data/img_data/SVID\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

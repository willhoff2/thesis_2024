{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a27e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "## Trim down imports to only neccesary \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, ViTModel\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "\n",
    "## Preprocessing loc\n",
    "preprocess_dir = '/Users/willhoff/Desktop/thesis_2024/preprocessing'\n",
    "sys.path.append(preprocess_dir)\n",
    "from simple_datasets import load_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa4b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretrained model\n",
    "\n",
    "class CustomViTEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomViTEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Extract the necessary layers from the original model\n",
    "        self.embeddings = original_model.vit.embeddings  #.patch_embeddings\n",
    "        self.encoder_layer_0 = original_model.vit.encoder.layer[0]\n",
    "        self.encoder_layer_1 = original_model.vit.encoder.layer[1]\n",
    "        \n",
    "        # Assume a square grid of patches to reshape the sequence of patches back into a 2D grid\n",
    "            ## image: 224x224 ; patch size: 16x16 --> 14x14 \n",
    "        self.num_patches_side = 14\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the embeddings layer\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # Pass the result through the first and second encoder layers\n",
    "        x = self.encoder_layer_0(x)[0]  # [0] to get the hidden states\n",
    "        x = self.encoder_layer_1(x)[0]  # [0] to get the hidden states\n",
    "        \n",
    "        # x is now the sequence of embeddings for the patches\n",
    "            # The output x will be a sequence of embeddings, one for each patch of the input images.\n",
    "            # If you're looking for a single vector representation per image, typically the class token embedding (the first token) is used. \n",
    "            # If the model doesn't use a class token, you might need to apply a different pooling strategy over the patch embeddings.\n",
    "        \n",
    "        ## Updating to reshape\n",
    "        \n",
    "        # Before reshaping, x is in shape [batch_size, num_patches+1, embedding_dim]\n",
    "        # We discard the first token which is used for classification in the original ViT model\n",
    "        x = x[:, 1:, :]  # Now in shape [batch_size, num_patches, embedding_dim]\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches_side, num_patches_side, embedding_dim]\n",
    "        x = x.reshape(-1, self.num_patches_side, self.num_patches_side, x.size(-1))\n",
    "\n",
    "        # Permute to get [batch_size, embedding_dim, num_patches_side, num_patches_side]\n",
    "        # This is a pseudo-spatial 2D grid, where embedding_dim becomes the channel dimension\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eabc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding in CNN component\n",
    "\n",
    "\n",
    "## CNN for regression\n",
    "class RegressionCNN(nn.Module):\n",
    "    ## Maybe try messing w/ kernel_size, padding and stride?\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(RegressionCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "        # Second convolutional layer\n",
    "            ## Same amount? Dropout should help it to learn other things\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "        # Third convolutional layer -- new\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        # Implementing dropout\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Adaptive pooling layer to pool down to 1x1\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Final fully connected layer for regressin\n",
    "        self.fc = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, batch norm, and activation\n",
    "        x = self.sig1(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolution, batch norm, and activation\n",
    "        x = self.sig2(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Apply third convolution, batch norm, and activation -- new\n",
    "        x = self.sig3(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pool the output from the convolutional layers\n",
    "        x = self.adapt_pool(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layer for regression\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def calculate_rmse_and_r2(loader, model, scaler):\n",
    "    model.eval()\n",
    "    targets, predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            preds = cnn_model(embeddings)  # Pass embeddings to the CNN\n",
    "            predictions.extend(preds.view(-1).tolist())\n",
    "            targets.extend(labels.tolist())\n",
    "\n",
    "    # Scale the targets using the provided scaler\n",
    "    targets_scaled = scaler.transform(np.array(targets).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert to tensors\n",
    "    predictions = torch.tensor(predictions)\n",
    "    targets_scaled = torch.tensor(targets_scaled)\n",
    "\n",
    "    # Calculate RMSE on scaled targets\n",
    "    rmse_value = torch.sqrt(nn.functional.mse_loss(predictions, targets_scaled))\n",
    "\n",
    "    # Calculate R^2 on scaled targets\n",
    "    r2_value = r2_score(targets_scaled, predictions)\n",
    "\n",
    "    return rmse_value.item(), r2_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431113a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, custom_model, scaler, return_depths=False, return_source = False):\n",
    "    model.eval()\n",
    "    predictions, actuals, depths,sources = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            indices, images, labels, batch_depths, batch_source = batch\n",
    "            embeddings = custom_model(images)\n",
    "            preds = model(embeddings)\n",
    "            predictions.extend(preds.view(-1).cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "            if return_depths:\n",
    "                depths.extend(batch_depths.cpu().numpy())\n",
    "            if return_source:\n",
    "                sources.extend(batch_source)\n",
    "\n",
    "    # Scale back if necessary\n",
    "    if scaler:\n",
    "        predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "        actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n",
    "\n",
    "    return sources, depths, actuals, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680e913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, title, save_path):\n",
    "    plt.figure(figsize=(10, 9))\n",
    "\n",
    "    # Convert to numpy arrays if not already\n",
    "    train_depths = np.array(train_depths)\n",
    "    train_actuals = np.array(train_actuals)\n",
    "    train_sources = np.array(train_sources)\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    test_depths = np.array(test_depths)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "    test_sources = np.array(test_sources)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "\n",
    "    # Create a grid with 2 rows and 1 column, \n",
    "    # the first row is twice the height of the second row\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1]) \n",
    "\n",
    "    # Main plot for actual values\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter train and test data\n",
    "        train_mask = (train_sources == lake)\n",
    "        test_mask = (test_sources == lake)\n",
    "        \n",
    "        #print(f\"{lake} - Train mask sum: {np.sum(train_mask)}, Test mask sum: {np.sum(test_mask)}\")\n",
    "\n",
    "        # Check and concatenate\n",
    "        if np.any(train_mask) or np.any(test_mask):\n",
    "            all_depths = np.concatenate([train_depths[train_mask], test_depths[test_mask]])\n",
    "            all_actuals = np.concatenate([train_actuals[train_mask], test_actuals[test_mask]])\n",
    "            #print(\"Check\\n\", all_depths, \"\\n\", all_actuals)\n",
    "            sorted_indices = np.argsort(all_depths)\n",
    "            plt.plot(all_depths[sorted_indices], all_actuals[sorted_indices], label=f'{lake} Actual', linestyle='-', marker='o')\n",
    "\n",
    "    ax1.set_xlabel('Sediment Depth')\n",
    "    ax1.set_ylabel('Target Variable')\n",
    "    ax1.set_title(title)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Subplot for residuals\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter train and test data for residuals\n",
    "        train_mask = (train_sources == lake)\n",
    "        test_mask = (test_sources == lake)\n",
    "\n",
    "        # Check and plot residuals\n",
    "        if np.any(train_mask):\n",
    "            train_depths_lake = train_depths[train_mask]\n",
    "            train_residuals = abs(train_actuals[train_mask] - train_predictions[train_mask])\n",
    "            sorted_indices = np.argsort(train_depths_lake)\n",
    "            plt.plot(train_depths_lake[sorted_indices], train_residuals[sorted_indices], label=f'{lake} Train Residual', linestyle='-', marker='x')\n",
    "\n",
    "        if np.any(test_mask):\n",
    "            test_depths_lake = test_depths[test_mask]\n",
    "            test_residuals = abs(test_actuals[test_mask] - test_predictions[test_mask])\n",
    "            sorted_indices = np.argsort(test_depths_lake)\n",
    "            plt.plot(test_depths_lake[sorted_indices], test_residuals[sorted_indices], label=f'{lake} Test Residual', linestyle='--', marker='o')\n",
    "\n",
    "    ax2.set_xlabel('Sediment Depth')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def old_plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, title, save_path):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Convert sources to numpy arrays if they are not already\n",
    "    train_depths = np.array(train_depths)\n",
    "    train_actuals = np.array(train_actuals)\n",
    "    train_sources = np.array(train_sources)\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    test_depths = np.array(test_depths)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_sources = np.array(test_sources)\n",
    "\n",
    "    check = 0\n",
    "    for lake in ['LVID', 'SVID']:\n",
    "        # Filter and check train data\n",
    "        lake_train_mask = train_sources == lake\n",
    "        if np.any(lake_train_mask):\n",
    "            lake_train_depths = train_depths[lake_train_mask]\n",
    "            lake_train_actuals = train_actuals[lake_train_mask]\n",
    "            sorted_train_indices = np.argsort(lake_train_depths)\n",
    "            plt.plot(lake_train_depths[sorted_train_indices], lake_train_actuals[sorted_train_indices], label=f'{lake} Train', linestyle='-', marker='o')\n",
    "            check += 1\n",
    "\n",
    "        # Filter and check test data\n",
    "        lake_test_mask = test_sources == lake\n",
    "        if np.any(lake_test_mask):\n",
    "            lake_test_depths = test_depths[lake_test_mask]\n",
    "            lake_test_actuals = test_actuals[lake_test_mask]\n",
    "            lake_test_predictions = test_predictions[lake_test_mask]\n",
    "            sorted_test_indices = np.argsort(lake_test_depths)\n",
    "            plt.plot(lake_test_depths[sorted_test_indices], lake_test_actuals[sorted_test_indices], label=f'{lake} Test Actual', linestyle='-', marker='o')\n",
    "            plt.plot(lake_test_depths[sorted_test_indices], lake_test_predictions[sorted_test_indices], label=f'{lake} Test Predicted', linestyle='--', marker='x')\n",
    "            check += 1\n",
    "            \n",
    "    if check == 0:\n",
    "        return \"No data to plot!\"\n",
    "    \n",
    "    plt.xlabel('Sediment Depth')\n",
    "    plt.ylabel('Target Variable')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23374e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_loader, test_loader, cnn_model, custom_model, scaler, num_epochs=100, lr=0.001):\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training steps...\n",
    "        cnn_model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch[1], batch[2]\n",
    "            print(images.shape)\n",
    "            embeddings = custom_model(images)\n",
    "            predictions = cnn_model(embeddings)\n",
    "            loss = loss_function(predictions.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    train_sources, train_depths, train_actuals, train_predictions = evaluate_model(cnn_model, train_loader, custom_model, scaler, return_depths=True, return_source = True)\n",
    "    test_sources, test_depths, test_actuals, test_predictions = evaluate_model(cnn_model, test_loader, custom_model, scaler, return_depths=True, return_source = True)\n",
    "\n",
    "    return train_depths, train_actuals, train_sources, train_predictions, test_depths, test_actuals, test_predictions, test_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccf6dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for Target: %toc, Train: svid, Test: svid\n",
      "%toc\n",
      "(<torch.utils.data.dataloader.DataLoader object at 0x3238f5650>, <torch.utils.data.dataloader.DataLoader object at 0x323940390>, None)\n",
      "tensor([[[[ 85.,  91.,  81.],\n",
      "          [ 85.,  88.,  79.],\n",
      "          [ 87.,  90.,  81.],\n",
      "          ...,\n",
      "          [ 83.,  88.,  81.],\n",
      "          [ 86.,  91.,  84.],\n",
      "          [ 82.,  87.,  80.]],\n",
      "\n",
      "         [[ 86.,  92.,  82.],\n",
      "          [ 86.,  89.,  80.],\n",
      "          [ 88.,  91.,  82.],\n",
      "          ...,\n",
      "          [ 85.,  90.,  83.],\n",
      "          [ 83.,  88.,  81.],\n",
      "          [ 87.,  92.,  85.]],\n",
      "\n",
      "         [[ 84.,  89.,  82.],\n",
      "          [ 84.,  87.,  80.],\n",
      "          [ 85.,  88.,  81.],\n",
      "          ...,\n",
      "          [ 85.,  90.,  83.],\n",
      "          [ 87.,  92.,  85.],\n",
      "          [ 83.,  88.,  81.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 13.,  12.,   8.],\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          ...,\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 15.,  14.,  10.]],\n",
      "\n",
      "         [[ 16.,  15.,  11.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 13.,  12.,   8.],\n",
      "          ...,\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 15.,  14.,  10.]],\n",
      "\n",
      "         [[ 19.,  18.,  14.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          ...,\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 15.,  14.,  10.]]],\n",
      "\n",
      "\n",
      "        [[[ 89.,  92.,  85.],\n",
      "          [ 88.,  91.,  84.],\n",
      "          [ 88.,  91.,  84.],\n",
      "          ...,\n",
      "          [ 87.,  94.,  86.],\n",
      "          [ 87.,  94.,  86.],\n",
      "          [ 86.,  93.,  85.]],\n",
      "\n",
      "         [[ 90.,  93.,  86.],\n",
      "          [ 89.,  92.,  85.],\n",
      "          [ 89.,  92.,  85.],\n",
      "          ...,\n",
      "          [ 85.,  92.,  84.],\n",
      "          [ 87.,  94.,  86.],\n",
      "          [ 85.,  92.,  84.]],\n",
      "\n",
      "         [[ 84.,  89.,  82.],\n",
      "          [ 85.,  90.,  83.],\n",
      "          [ 89.,  92.,  85.],\n",
      "          ...,\n",
      "          [ 88.,  95.,  87.],\n",
      "          [ 89.,  96.,  88.],\n",
      "          [ 87.,  94.,  86.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 13.,  12.,  10.],\n",
      "          [ 12.,  11.,   9.],\n",
      "          [ 12.,  11.,   9.],\n",
      "          ...,\n",
      "          [ 11.,  11.,   9.],\n",
      "          [ 14.,  14.,  12.],\n",
      "          [ 15.,  15.,  13.]],\n",
      "\n",
      "         [[ 13.,  12.,  10.],\n",
      "          [ 12.,  11.,   9.],\n",
      "          [ 12.,  11.,   9.],\n",
      "          ...,\n",
      "          [ 10.,  10.,   8.],\n",
      "          [ 10.,  10.,   8.],\n",
      "          [ 11.,  11.,   9.]],\n",
      "\n",
      "         [[ 12.,  11.,   9.],\n",
      "          [ 12.,  11.,   9.],\n",
      "          [ 13.,  12.,  10.],\n",
      "          ...,\n",
      "          [ 13.,  13.,  11.],\n",
      "          [ 11.,  11.,   9.],\n",
      "          [ 11.,  11.,   9.]]],\n",
      "\n",
      "\n",
      "        [[[ 93.,  98.,  91.],\n",
      "          [ 95., 100.,  93.],\n",
      "          [ 94.,  99.,  92.],\n",
      "          ...,\n",
      "          [ 91.,  97.,  87.],\n",
      "          [ 89.,  95.,  85.],\n",
      "          [ 91.,  97.,  87.]],\n",
      "\n",
      "         [[ 90.,  95.,  88.],\n",
      "          [ 90.,  95.,  88.],\n",
      "          [ 88.,  93.,  86.],\n",
      "          ...,\n",
      "          [ 92.,  98.,  88.],\n",
      "          [ 89.,  95.,  85.],\n",
      "          [ 84.,  92.,  81.]],\n",
      "\n",
      "         [[ 90.,  95.,  88.],\n",
      "          [ 89.,  94.,  87.],\n",
      "          [ 88.,  93.,  86.],\n",
      "          ...,\n",
      "          [ 89.,  95.,  85.],\n",
      "          [ 88.,  96.,  85.],\n",
      "          [ 89.,  97.,  86.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 17.,  16.,  12.],\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 17.,  16.,  12.],\n",
      "          ...,\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 15.,  14.,  10.]],\n",
      "\n",
      "         [[ 17.,  16.,  12.],\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          ...,\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.]],\n",
      "\n",
      "         [[ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          ...,\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 18.,  17.,  13.],\n",
      "          [ 18.,  17.,  13.]]],\n",
      "\n",
      "\n",
      "        [[[ 21.,  12.,   7.],\n",
      "          [ 23.,  14.,  17.],\n",
      "          [ 18.,  12.,  22.],\n",
      "          ...,\n",
      "          [ 16.,  19.,   0.],\n",
      "          [ 21.,  18.,  13.],\n",
      "          [ 20.,  18.,  23.]],\n",
      "\n",
      "         [[ 19.,  18.,  13.],\n",
      "          [ 17.,  17.,  15.],\n",
      "          [ 17.,  19.,  18.],\n",
      "          ...,\n",
      "          [ 12.,  22.,  13.],\n",
      "          [ 14.,  14.,  12.],\n",
      "          [ 17.,  11.,  15.]],\n",
      "\n",
      "         [[ 15.,  18.,  11.],\n",
      "          [ 15.,  18.,  11.],\n",
      "          [ 13.,  16.,   7.],\n",
      "          ...,\n",
      "          [ 11.,  28.,  20.],\n",
      "          [ 17.,  22.,  18.],\n",
      "          [ 19.,  13.,  13.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 14.,  13.,  11.],\n",
      "          [ 13.,  12.,  10.],\n",
      "          [ 13.,  12.,  10.],\n",
      "          ...,\n",
      "          [ 14.,  13.,  11.],\n",
      "          [ 14.,  13.,  11.],\n",
      "          [ 15.,  14.,  12.]],\n",
      "\n",
      "         [[ 15.,  14.,  12.],\n",
      "          [ 15.,  14.,  12.],\n",
      "          [ 13.,  12.,  10.],\n",
      "          ...,\n",
      "          [ 14.,  13.,  11.],\n",
      "          [ 15.,  14.,  12.],\n",
      "          [ 16.,  15.,  13.]],\n",
      "\n",
      "         [[ 14.,  13.,  11.],\n",
      "          [ 16.,  15.,  13.],\n",
      "          [ 13.,  12.,  10.],\n",
      "          ...,\n",
      "          [ 14.,  13.,  11.],\n",
      "          [ 15.,  14.,  12.],\n",
      "          [ 16.,  15.,  13.]]],\n",
      "\n",
      "\n",
      "        [[[ 16.,  15.,  10.],\n",
      "          [ 17.,  16.,  11.],\n",
      "          [ 16.,  15.,  10.],\n",
      "          ...,\n",
      "          [ 18.,  17.,  12.],\n",
      "          [ 18.,  17.,  12.],\n",
      "          [ 19.,  18.,  13.]],\n",
      "\n",
      "         [[ 20.,  19.,  14.],\n",
      "          [ 18.,  17.,  12.],\n",
      "          [ 16.,  15.,  10.],\n",
      "          ...,\n",
      "          [ 17.,  16.,  11.],\n",
      "          [ 17.,  16.,  11.],\n",
      "          [ 18.,  17.,  12.]],\n",
      "\n",
      "         [[ 16.,  15.,  10.],\n",
      "          [ 15.,  14.,   9.],\n",
      "          [ 17.,  16.,  11.],\n",
      "          ...,\n",
      "          [ 19.,  18.,  13.],\n",
      "          [ 19.,  18.,  13.],\n",
      "          [ 20.,  19.,  14.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 12.,  11.,   7.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 17.,  16.,  12.],\n",
      "          ...,\n",
      "          [ 17.,  16.,  12.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          [ 13.,  12.,   8.]],\n",
      "\n",
      "         [[ 15.,  14.,  10.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 16.,  15.,  11.],\n",
      "          ...,\n",
      "          [ 15.,  14.,  10.],\n",
      "          [ 14.,  13.,   9.],\n",
      "          [ 12.,  11.,   7.]],\n",
      "\n",
      "         [[ 19.,  18.,  14.],\n",
      "          [ 18.,  17.,  13.],\n",
      "          [ 15.,  14.,  10.],\n",
      "          ...,\n",
      "          [ 13.,  12.,   8.],\n",
      "          [ 13.,  12.,   8.],\n",
      "          [ 14.,  13.,   9.]]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 1300.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m train_loader, _, train_scaler \u001b[38;5;241m=\u001b[39m load_data(target, lake\u001b[38;5;241m=\u001b[39mtrain_set, \u001b[38;5;28mset\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, scaled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sediment_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m _, test_loader, _ \u001b[38;5;241m=\u001b[39m load_data(target, lake\u001b[38;5;241m=\u001b[39mtest_set, \u001b[38;5;28mset\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, scaled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sediment_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m train_depths, train_actuals, train_sources, train_predictions, test_depths, test_actuals, test_predictions, test_sources \u001b[38;5;241m=\u001b[39m run_experiment(train_loader, test_loader, cnn_model, custom_model, train_scaler, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#print(train_sources,test_sources)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(train_loader, test_loader, cnn_model, custom_model, scaler, num_epochs, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m], batch[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(images)\n\u001b[0;32m---> 14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m custom_model(images)\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m cnn_model(embeddings)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(predictions\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n",
      "File \u001b[0;32m~/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mCustomViTEmbeddingModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Apply the embeddings layer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(x)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Pass the result through the first and second encoder layers\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layer_0(x)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# [0] to get the hidden states\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:122\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    118\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    121\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 122\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:171\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    169\u001b[0m batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 1300."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "## Define the combinations\n",
    "combinations = [\n",
    "    #(\"lvid\", \"lvid\"),\n",
    "    #(\"lvid\", \"svid\"),\n",
    "    #(\"lvid\", \"both\"),\n",
    "    #(\"svid\", \"lvid\"),\n",
    "    (\"svid\", \"svid\"),\n",
    "    #(\"svid\", \"both\"),\n",
    "    #(\"both\", \"lvid\"),\n",
    "    #(\"both\", \"svid\"),\n",
    "    #(\"both\", \"both\")\n",
    "]\n",
    "targets = [\"%toc\"]\n",
    "# Load the pre-trained ViT model\n",
    "pretrained_vit = ViTForImageClassification.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "\n",
    "## Freeze params\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create model w first three layers and create embedding\n",
    "custom_model = CustomViTEmbeddingModel(pretrained_vit)\n",
    "\n",
    "for target in targets:\n",
    "    for train_set, test_set in combinations:\n",
    "        cnn_model = RegressionCNN(embedding_dim=192)\n",
    "        print(f\"Running experiment for Target: {target}, Train: {train_set}, Test: {test_set}\")\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, _, train_scaler = load_data(target, lake=train_set, set=\"full\", scaled=False, sediment_width = 1)\n",
    "        _, test_loader, _ = load_data(target, lake=test_set, set=\"full\", scaled=False, sediment_width = 1)\n",
    "        \n",
    "        train_depths, train_actuals, train_sources, train_predictions, test_depths, test_actuals, test_predictions, test_sources = run_experiment(train_loader, test_loader, cnn_model, custom_model, train_scaler, num_epochs=100, lr=0.001)\n",
    "        #print(train_sources,test_sources)\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H-%M-%S\")\n",
    "        save_dir = 'data_exp/1cm/plots'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = f'{target}_{train_set}train{test_set}_test_{timestamp}.png'\n",
    "        plot_title = f\"{target} Predictions (Train: {train_set}, Test: {test_set})\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        plot_results(train_depths, train_actuals, train_predictions, train_sources,  test_depths, test_actuals, test_predictions, test_sources, plot_title, save_path)\n",
    "        filename = f'OLD{target}_{train_set}train{test_set}_test_{timestamp}.png'\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        old_plot_results(train_depths, train_actuals, train_predictions, train_sources, test_depths, test_actuals, test_predictions, test_sources, plot_title, save_path)\n",
    "        # Calculate performance metrics\n",
    "        rmse_value = np.sqrt(mean_squared_error(test_actuals, test_predictions))\n",
    "\n",
    "        # Add the results to the results dataframe\n",
    "        results.append({\n",
    "            \"Train Set\": train_set,\n",
    "            \"Test Set\": test_set,\n",
    "            \"Target\": target,\n",
    "            \"RMSE\": rmse_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d-%H-%M-%S\")\n",
    "save_dir = 'data_exp/1cm/results'\n",
    "\n",
    "# Saving the plot\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "filename = f'experiment_results_{timestamp}.csv'\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(os.path.join(save_dir, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls \"/Users/willhoff/Desktop/thesis_2024/data/img_data/SVID\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

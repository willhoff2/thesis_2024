{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2038fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/willhoff/Desktop/research_23_24/research_09_11/env/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Trim down imports to only neccesary \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, ViTModel\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import datasets_1 as datasets\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from simple_datasets import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449193fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for VIT: cpu\n"
     ]
    }
   ],
   "source": [
    "## Pretrained model\n",
    "\n",
    "class CustomViTEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomViTEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Extract the necessary layers from the original model\n",
    "        self.embeddings = original_model.vit.embeddings  #.patch_embeddings\n",
    "        self.encoder_layer_0 = original_model.vit.encoder.layer[0]\n",
    "        self.encoder_layer_1 = original_model.vit.encoder.layer[1]\n",
    "        \n",
    "        # Assume a square grid of patches to reshape the sequence of patches back into a 2D grid\n",
    "            ## image: 224x224 ; patch size: 16x16 --> 14x14 \n",
    "        self.num_patches_side = 14\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the embeddings layer\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # Pass the result through the first and second encoder layers\n",
    "        x = self.encoder_layer_0(x)[0]  # [0] to get the hidden states\n",
    "        x = self.encoder_layer_1(x)[0]  # [0] to get the hidden states\n",
    "        \n",
    "        # x is now the sequence of embeddings for the patches\n",
    "            # The output x will be a sequence of embeddings, one for each patch of the input images.\n",
    "            # If you're looking for a single vector representation per image, typically the class token embedding (the first token) is used. \n",
    "            # If the model doesn't use a class token, you might need to apply a different pooling strategy over the patch embeddings.\n",
    "        \n",
    "        ## Updating to reshape\n",
    "        \n",
    "        # Before reshaping, x is in shape [batch_size, num_patches+1, embedding_dim]\n",
    "        # We discard the first token which is used for classification in the original ViT model\n",
    "        x = x[:, 1:, :]  # Now in shape [batch_size, num_patches, embedding_dim]\n",
    "        \n",
    "        # Reshape to [batch_size, num_patches_side, num_patches_side, embedding_dim]\n",
    "        x = x.reshape(-1, self.num_patches_side, self.num_patches_side, x.size(-1))\n",
    "\n",
    "        # Permute to get [batch_size, embedding_dim, num_patches_side, num_patches_side]\n",
    "        # This is a pseudo-spatial 2D grid, where embedding_dim becomes the channel dimension\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def calculate_rmse_and_r2(loader, model, scaler, scaled = True):\n",
    "    ## Should probably make device a param for consistency\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    targets, predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch[0].to(device), batch[1].to(device)\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            preds = model(embeddings)  # Pass embeddings to the CNN\n",
    "            predictions.extend(preds.view(-1).cpu().numpy())  # Transfer to CPU for numpy compatibility\n",
    "            targets.extend(labels.cpu().numpy())  # Transfer to CPU for numpy compatibility\n",
    "\n",
    "    # Convert to tensors\n",
    "    predictions = torch.tensor(predictions).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    if scaled:\n",
    "        # Move targets to CPU for scaling, as scaler works on CPU\n",
    "        targets_scaled = scaler.transform(targets.cpu().numpy().reshape(-1, 1)).flatten()\n",
    "        targets_scaled = torch.tensor(targets_scaled).to(device)  # Move back to GPU\n",
    "\n",
    "        # Calculate RMSE on scaled targets\n",
    "        rmse_value = torch.sqrt(nn.functional.mse_loss(predictions, targets_scaled))\n",
    "\n",
    "        # Calculate R^2 on scaled targets (requires CPU operation)\n",
    "        r2_value = r2_score(targets_scaled.cpu(), predictions.cpu())\n",
    "    else:\n",
    "        # Calculate RMSE on unscaled targets\n",
    "        rmse_value = torch.sqrt(nn.functional.mse_loss(predictions, targets))\n",
    "\n",
    "        # Calculate R^2 on unscaled targets (requires CPU operation)\n",
    "        r2_value = r2_score(targets.cpu(), predictions.cpu())\n",
    "\n",
    "    return rmse_value.item(), r2_value\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device for VIT: {device}\")\n",
    "# Load the pre-trained ViT model\n",
    "pretrained_vit = ViTForImageClassification.from_pretrained('facebook/deit-tiny-patch16-224').to(device)\n",
    "\n",
    "## Freeze params\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "# Create model w first three layers and create embedding\n",
    "custom_model = CustomViTEmbeddingModel(pretrained_vit).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965907a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preventing model arch from printing everytime\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e696da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search\n",
    "    ## Second grid search\n",
    "\n",
    "class GridRegressionCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, layer_sizes, use_dropout=True, use_batch_norm=True, act_func=nn.ReLU()):\n",
    "        super(GridRegressionCNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = embedding_dim\n",
    "\n",
    "        for size in layer_sizes:\n",
    "            layers.append(nn.Conv2d(in_channels, size, kernel_size=3, padding=1))\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(size))\n",
    "            \n",
    "            layers.append(act_func)\n",
    "\n",
    "            if use_dropout:\n",
    "                ## Changed to 0.1 on 12/19\n",
    "                layers.append(nn.Dropout(p=0.1))\n",
    "\n",
    "            in_channels = size\n",
    "\n",
    "        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "        ## convert list into sequential\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=layer_sizes[-1], out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "\n",
    "        ## flattening tensor for fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7c81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_evaluate_model(params,train_loader,val_loader,scaler, full_labels, scaled = False):\n",
    "    \n",
    "    ## Maybe have there be a check for whether were using a gpu?\n",
    "    \n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device in train_evaluate: {device}\")\n",
    "    \n",
    "    \n",
    "    model = GridRegressionCNN(embedding_dim=192, layer_sizes=params['layer_sizes'], use_dropout=params['use_dropout'], use_batch_norm=params['use_batch_norm'],act_func = params['activations']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # Define LR scheduler\n",
    "    if params['use_lr_decay']:\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)  # Adjust these parameters as needed\n",
    "    \n",
    "    # Reset metrics for each training session\n",
    "    train_losses = []\n",
    "    train_rmses = []\n",
    "    val_rmses = []\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        #print(f'epoch {epoch} of {epochs}')\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images, labels = batch[0].to(device), batch[1].to(device)\n",
    "            embeddings = custom_model(images)  # Get embeddings from the ViT\n",
    "            predictions = model(embeddings)  # Pass embeddings to the CNN\n",
    "            loss = loss_function(predictions.squeeze(), labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if params['use_lr_decay']:\n",
    "                scheduler.step()  # Update the learning rate\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Append average loss and RMSE for this epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_rmse, _ = calculate_rmse_and_r2(train_loader, model, scaler, scaled)\n",
    "        val_rmse, _ = calculate_rmse_and_r2(val_loader, model, scaler, scaled)\n",
    "        train_rmses.append(train_rmse)\n",
    "        val_rmses.append(val_rmse)\n",
    "    \n",
    "    ## Changed range_target to mean -- 01/07/24\n",
    "    range_target = torch.mean(full_labels)\n",
    "    if scaled:\n",
    "        normal_rmse = scaler.inverse_transform([[val_rmses[-1]]])[0, 0]\n",
    "        rmse_perc = (normal_rmse / range_target.item()) * 100\n",
    "    else:\n",
    "        rmse_perc = (val_rmses[-1] / range_target.item()) * 100\n",
    "    print(f'PERFORMANCE: \\n params: {params}, \\n val_rmse: {val_rmses[-1]} \\n rmse_perc: {rmse_perc} \\n')\n",
    "\n",
    "    return {'train_rmse': train_rmses[-1], 'val_rmse': val_rmses[-1], 'rmse_perc': rmse_perc}\n",
    "\n",
    "def manual_grid_search(lake,target,param_grid, train_loader, val_loader,scaler, labels, scaled = False):\n",
    "    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    results = []\n",
    "    for i, params in enumerate(all_params):\n",
    "        epochs = params['epochs']\n",
    "        print(f'run {i+1} of {len(all_params)}')\n",
    "        performance = train_evaluate_model(params, train_loader, val_loader,scaler, labels, scaled)\n",
    "        results.append({'params': params, 'performance': performance})\n",
    "\n",
    "    # Convert results to a DataFrame for easier handling\n",
    "    results_df = pd.DataFrame([{\n",
    "        'epochs': r['params']['epochs'],\n",
    "        'learning_rate': r['params']['learning_rate'],\n",
    "        'layer_sizes': r['params']['layer_sizes'],\n",
    "        'use_dropout': r['params']['use_dropout'],\n",
    "        'use_batch_norm': r['params']['use_batch_norm'],\n",
    "        'use_lr_decay': r['params']['use_lr_decay'],\n",
    "        'activation': r['params']['activations'],\n",
    "        'train_rmse': r['performance']['train_rmse'],\n",
    "        'val_rmse': r['performance']['val_rmse'],\n",
    "        'RMSE %': r['performance']['rmse_perc']\n",
    "    } for r in results])\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    target = target.replace(\"/\", \"-\")\n",
    "    title = \"grid_results/\" + lake + \"/\" + target + \"_\" + date.today().strftime(\"%d%m%Y_%H%M%S\") + '_grid_search_results.csv'\n",
    "    print(f'saved to {title}')\n",
    "    results_df.to_csv(title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf85353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN MBT\n",
      "\n",
      "run 1 of 210\n",
      "Using device in train_evaluate: cpu\n",
      "PERFORMANCE: \n",
      " params: {'learning_rate': 1e-08, 'layer_sizes': [256, 64], 'use_dropout': True, 'use_batch_norm': True, 'use_lr_decay': True, 'activations': Sigmoid(), 'epochs': 100}, \n",
      " val_rmse: 1.0323017835617065 \n",
      " rmse_perc: 622.0981955842329 \n",
      "\n",
      "run 2 of 210\n",
      "Using device in train_evaluate: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Warnings off\n",
    "## Preventing model arch from printing everytime\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.00000001, 0.0000005, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001],\n",
    "    'layer_sizes': [[256,64], [128,64], [256,128,64], [512,256,64], [512,256,128,64]],\n",
    "    'use_dropout': [True],\n",
    "    'use_batch_norm': [True],\n",
    "    'use_lr_decay': [True,False],\n",
    "    'activations': [nn.Sigmoid()],\n",
    "    'epochs': [100,200,500]\n",
    "}\n",
    "\n",
    "targets = ['MBT']\n",
    "lake = 'VIDs'\n",
    "for target in targets:\n",
    "    print(f'BEGIN {target}\\n')\n",
    "    if target == \"%TOC\":\n",
    "        train_loader, val_loader, scaler,labels_tensor = load_data('%TOC', lake=\"both\", set=\"full\", scaled=False, return_labels = True)\n",
    "    else:\n",
    "        train_loader, val_loader, scaler, labels_tensor = load_data('MBT', lake=\"both\", set=\"full\", scaled=False, return_labels = True)\n",
    "    \n",
    "        \n",
    "    ## Do grid search in function\n",
    "    manual_grid_search(lake, target, param_grid, train_loader, val_loader,scaler, labels_tensor, scaled = False)\n",
    "    print(f'FINISHED {target}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
